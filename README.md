# Bench4ComPat
Comprehensive Benchmark Dataset for HPC Computational Patterns

1. **Dense Linear Algebra:** This consists of dense matrix and vector operations. It has a high ratio of math-to-load operations and a high degree of data interdependency between threads. We are finalizing a benchmark for this dwarf based on LU factorization, but for the time being, we include the k-means clustering algorithm, denoted as `kmeans` in OCD.
2. **Sparse Linear Algebra:** This solves the same problem as dense linear algebra but has matrices with few non-zero entries. To reduce space and computation, such algorithms store and operate on a list of values and indices rather than proper matrices, resulting in more indirect memory accesses. For OCD, we implement a pattern for matrix-vector multiplication that uses a compressed sparse row format to store sparse matrices. As such, the implemented dwarf is denoted as `csr`.
3. **Spectral Methods:** These transform data from/to either a spatial or temporal domain. The execution profile is typically characterized by multiple stages of processing, where dependencies within a stage form a “butterfly” pattern of computation. We capture this pattern via a FFT, i.e., `clfft` in OCD.
4. **N-body Methods:** These calculate interactions between many discrete points and are characterized by large numbers of independent calculations within a timestep, followed by all-to-all communication between timesteps. Our GEM code, denoted as `gemnoui` in OCD, calculates the effect that all atoms have on the charge at each point along the surface of a molecule, leading to \(O(M \cdot N)\) complexity where \(N\) is atoms and \(M\) is points along the surface.
5. **Structured Grids:** These organize data in a regular multidimensional grid, where computation proceeds as a series of grid updates. For each grid update, all points are updated using values from a small neighborhood around each point. The neighborhood is normally implicit in the data and determined by the algorithm. For OCD, we include `srad`, short for speckle-reducing anisotropic diffusion, a stencil-based pattern of computation and communication that reduces noise and enhances feature clarity in 2D images.
6. **Unstructured Grids:** These possess data structures, e.g., linked lists of pointers, that keep track of the location and ‘neighborhood’ of points which are used to update the location. Like sparse linear algebra, updates typically involve multiple levels of memory reference indirection, as an update to any point requires first determining a list of neighboring points, and then loading values from those neighboring points. For OCD, we include a pattern of computation and communication that is representative of an unstructured grid code for computational fluid dynamics, denoted as `cfd` in OCD.
7. **MapReduce:** This captures the repeated independent execution of a “map” function and results are aggregated at the end via a “reduce” function. No communication is required between processes in the map phase, but the reduce phase requires global communication. For OCD, we have a prototype dwarf that we dub `StreamMR` (“streamer”).
8. **Combinational Logic:** This exploits bit-level parallelism to achieve high throughput. Such a workload involves performing simple operations on very large amounts of data. For OCD, we include `crc`, short for cyclic redundancy check, which is used to generate hashes or signatures of files to verify their correct transfer over a network.
9. **Graph Traversal:** This visits and evaluates a number of objects in a graph. Such algorithms typically involve a significant amount of random memory access for indirect lookups. The bottleneck is generally due to access latency rather than access bandwidth. For OCD, we include breadth-first search (`bfs`) and bitonic sort (`bsort`).
10. **Dynamic Programming:** This solves a complex problem by solving a series of simpler subproblems. For OCD, we adopt the Needleman-Wunsch algorithm, i.e., `needle` in OCD. This algorithmic pattern calculates the optimal alignment of two strings by calculating scores based on all possible alignments in a matrix and backtracking along the highest scoring path.
11. **Backtrack & Branch-and-Bound:** These approaches generally search a very large search space to find a globally optimal solution. Because the search space is so large, an implicit method is needed to prune the search space to make this approach computationally tractable. For OCD, we capture the computation and communication pattern of the A* search algorithm (`astar` in OCD).
12. **Graphical Models:** These map variables into nodes and conditional probabilities into edges, e.g., Bayesian networks. For OCD, we have captured this pattern of computation and communication via a hidden Markov model.
13. **Finite State Machines:** These capture a system whose behavior is defined by states, transitions defined by inputs and the current state, and events associated with transitions or states. These dwarf algorithms are highly dependent on conditional operations and interdependent data, which are also commonly found in graph traversal. For OCD, we provide a “temporal data mining” algorithm, which discovers temporal correlations between EEG events from the brain.